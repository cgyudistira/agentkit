---
name: LLM Inference Optimization
description: Quantization, caching, batching, and serving optimization for LLM inference.
---

# LLM Inference Optimization

Quantization, caching, batching, and serving optimization for LLM inference.

## When to Use

Use this skill when working on ai engineer tasks related to llm inference optimization.

## Key Concepts

1. **Best Practices**: Follow industry standards
2. **Implementation**: Step-by-step guidance
3. **Examples**: Real-world applications

## Guidelines

- Start with understanding requirements
- Apply proven patterns
- Test and validate results
